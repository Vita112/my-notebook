------

> 强化学习简介：
>
> - 强化学习和监督学习非监督学习的区别
>
>   强化学习首先是需要类似于损失函数一样的东西去进行监督学习的，强化学习，监督学习，非监督学习的不同也是通过这个监督机制进行区分的。
>
>   - 监督学习：
>
>     模型有输入输出，且存在拥有确定输出的输入数据。
>
>   - 非监督学习
>
>     模型有输入有输出有输出，但对于模型进行监督的往往是某种规则。
>
>   - 强化学习
>
>     模型有输入有输出，存在一个独立于模型的机制对模型的预测作出评价。
>
> - 主要算法和分类
>
>   - Policy based, 关注点是找到最优策略
>   - Value based, 关注点是找到最优奖励总和
>   - Action based, 关注点是每一步的最优行动。
>   - **Model-based**：先理解真实世界是怎样的, 并建立一个模型来模拟现实世界的反馈
>
>   这里只简要介绍policy based model
>
> - Deep Policy based model
>
>   - Neural network as a Actor
>
>     这里的Actor就是处理输入的模型。
>
>     在这个模型中就是提出的reattention模型，输入是（问题，回答，文本）三元组。输出是答案。
>
>   - Goodness of the function
>
>     模型预测结果的评价，但是这里与一般的深度模型不同的是，强化学习认为模型的结果具有随机性，原因有两点：（这里只是根据课件给出的类似定义，暂时无法理解）
>
>     - Actor本身是随机的，相同情况下（Observation）可能采取不同的操作（Action）。
>     - 监督机制本身有随机性。即使Actor采取相同的操作（Action），监督机制可能给出不同的反馈（Observation）。
>
>     那么该如何评价模型预测的结果呢？
>
>     **求reward的期望值。下面是解释**
>
>     - 假设一个学习周期（episode）分为三步：
>
>       - start with the obversation s_i
>       - Machine decides to react a_i
>       - Machine obtains reward r_i
>
>     -  那么总reward为：
>
>       $R=\sum_{n=1}^Nr_n$ , 又由于这其中是有重复的周期，我们按照频率对其进行总结后，改进公式如下：
>
>       $R=\sum_{\epsilon =1}^NR(\epsilon)P(\epsilon|\theta)$
>
>       这里的 $\theta$ 就是模型中的参数，奖励的期望也就是：
>
>       $E(\epsilon) = \frac{1}{N}\sum_{\epsilon =1}^NR(\epsilon)P(\epsilon|\theta)$
>
>     - 接下来就可以对网络进行计算
>
>   - Pick the best function
>
>     - 原文地址(https://blog.csdn.net/qq_32690999/article/details/78594220)
>
>     - 最后推导出结果如下：
>
>       ![](./pictures/2.png)
>
>     - 由于这里是基于概率取样，而概率取样本身存在的随机性使得我们可能会在过程中从未sampling过某个可选的action，导致对应action的概率降低。
>
>     - 一个简单地解决上述sampling问题的方法是：令总Reward有负有正。即，当所有τ对应的总Reward都为正时，我们让它们统一减去某个正值（baseline），使这些Reward有正有负。这样就能防止某个action没被sample到，就会导致其概率减少。 **这里没有懂，之后再看**

-----





而这个方法的优越之处在于，这里选择动态的改变baseline和reward的值。