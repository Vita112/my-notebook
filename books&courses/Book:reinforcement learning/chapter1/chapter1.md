## 1 Reinforcement learning

### 1.1 Insight

强化学习指的是, agent 在没有任何指导的情况下, 在环境中做出反应接收反馈, 对自己行为做出调整的一个过程.

因此, 重要的在于环境的设立, 这个是相当重要的. 

### 1.2 其他

##### 1. 特性

- Trial-and-error : 实验和错误, 即根据实验修改自己行为
- 延迟奖励 : 奖励的获取可能是在一系列actions之后才可以获得的



##### 2. 问题与方法

书中说道, 明确RL中**问题**和**方法**的定义是很重要的.

第三章的MDP过程不是来解决问题的过程, 而是描述问题的方法

**问题** : 三个要素

- 感知 : 感知环境
- 动作 : 采取动作
- 目标 : 有一个目标, 和环境是有关的

拥有这样步骤的问题就叫做**强化学习问题.**

**方法** : 

可以解决上面的问题的方法就叫做**强化学习方法.**



##### 3. 和有监督无监督的区别

和有监督的很简单, 即没有显式地提供一个准确的label, 而是提供一个环境去让agent自己判断. 而强化学习能够从自己过去的经验中获得学习.

与无监督的比较难, 都是利用数据的特性去做一些事情, 但是RL有一个环境的设立. 无监督只是为了**获取数据内部结构**, 但是RL在此之上又去**和环境交互去获得反馈.**



##### 4. Trade-off

是位于 exploration-exploitation 之间的 trade-off.

Exploration : 即探索之前没有做过的动作

Exploitation : 即使用之前获得高reward的action.



##### 5. 处理整体问题的能力

###### 1)  整体问题与子问题

记得在看关系抽取的论文时, 其中讲到了 Jointly learning的方法, 其实其对应就是将一个问题分为几个子问题, 然后再一起学习的思想, 可以说这个相比于增量式学习是很有优势的, 但是他还是将问题拆开了来看. 

- 而即使在这个方法中, 也需要调整各个子问题的权重的超参数, 这个是非常主观的. 


- 并且, 子问题之间的交互由人为设定也是不太好的.

强化学习认为, 这样将**一整个问题分为几个独立的问题去思考是不好的**. 

- 并且能解决这个各部分之间重要性问题分析的算法(planning), 没有考虑实时性的问题. 

###### 2) 强化学习的"我偏要勉强"

面对复杂的拥有巨大不确定性的环境, 强化学习说, 我不管, 我就只有一个目标, 管你个山更险来水更恶, 我就要勉强!!

在与环境的交互中, 上面的**三个**问题都可以解决.



##### 6. Weak and Strong methods

这里没看懂, 主要是讨论, 强化学习是否可以处理更一般的原则. 

Weak method : 认为人工智能可以基于一般的方法完成, 没必要积攒成千上万的规则.

Strong method : 基于大量的知识与规则

强化学习无疑属于Weak method



### 1.2 Examples

看书, 都是一些与环境有进行交互的例子.

共同点:

- agent与环境之间有交互
- agent要对环境有一定观察或者检查
- 有一个明确的目标
- agent可以利用其既有经验去提成自己的performance.



### 1.3 强化学习的几个要素

#### 1.3.1 几个要素

##### 1. Policy

即, agent 采取动作的策略, 这个步骤的输入是环境的现有状态.

##### 2. Reward signal

在agent没做完一个action后, 都会从Environment那里获得Reward signal的反馈. 

##### 3. Value function

这个是在一个序列的动作完成后获得的结果.

##### 4. Model of Enviroment

对于环境的建模, 输入policy后的action 到环境中, 会反馈给agent Reward.

#### 1.3.2 交互

现阶段对环境的观测 $\to$ policy $\to$ action $\to$ Model of environment $\to$ reward

对上面的过程进行重复, 之后触发某个条件后, 得到 Value function.

#### 1.3.3 Reward和Value

Reward反映的是固有的, 立即的对于环境的可取性的评价.

Value反应的是, long-term地对于环境的可取性的评价. 

没有Reward就没有Value, 我们真正想要的是Value,最后的目的是得到最大的Value.

但是这个是很难的.

#### 1.3.4 Model的种类

有三个种类. 这里以强化学习中的马尔可夫决策过程MDP为例子. 关于MDP的介绍见[这里](https://blog.csdn.net/ppp8300885/article/details/78524235)

里面有四元组:

- $S$ : 环境的状态空间
- $A$：agent可选择的动作空间
- $R(s,a)$：奖励函数，返回的值表示在s状态下执行a动作的奖励
- $T(s'|s,a)$ : 状态转移概率函数，表示从s状态执行a动作后环境转移至s′状态的概率, **policy**

那么,  环境信息是储存在 $R(s,a)$ 和 $T(s'|s,a)$ 里面的.

这个其中也包含 根据 环境状态 $s$ 来制定 动作 $a$ 的信息, 就是通过最大化 $Value$ , 而$Value$ 是来源于 $R(s,a)$ 的.

##### 1. planning

这里面所有的值都固定了, 主要是指状态转移概率函数.

这个条件下, 环境是已知的.

在这种情况下, 环境的所有信息就已经包含进了奖励函数和状态转移概率函数. 因此, 只需要制定个策略找到最大的Value即可.  即制定, 根据 环境状态 $s$ 来制定 动作 $a$ 的模型.

##### 2. Model-based 

这种情况, 环境是未知的. 也就是说, $R(s,a)$ 和 $T(s'|s,a)$ 是未知的.

需要建立一个模型, 并且需要一些用于监督学习的数据, 告诉模型什么时候应该有什么反应. 已经是说, 接收现在的环境状态 $s_1$ 和 执行的动作 $a_1$ , 观察接下来的环境变化 $s_2$ 和收到的Reward $r$. 

也就是说 利用监督学习去对环境建模. 

> 这里的监督数据也可以来源于尝试数据这种可以获得最大Value的action序列.

之后收敛后, 可在运用planning去寻找最优方案.

##### 3. Model-Free

直接使环境状态 $s_1$ 和 执行的动作 $a_1$ 去预测最后收益, $Q(s_t,a_t)$, 如果这个预测模型精确度很高的情况下, 就可以直接用 $\operatorname{argmax}_{a_t}Q(s_t,a_t)$ 来求计算可以获取最大Value的动作.

这个最后不需要使用Planning, 因为这里是对最后的Value建的模.



### 1.4 限制 

本书局限:

- 这本书没有涉及重建, 改变以及学习状态信号的问题. 而是集中精力在action决策的方面.

- 不涉及evolutionary method问题:

  evolutionary method : 进化方法是指一些不需要value function的方法, 多采用利用生存时间作为反馈的类生物学进化的方法.

  这种方法相对于一般方法效率较低, 因此适用于更新迭代快的问题.

  并且这种方法适用于agent无法感知全部的环境的情况.

  这种方法无法使用很多agent生存过程中与环境的交互信息, 不是我们考虑的范围.

Scope:

- 强化学习方法是通过估计value函数进行构建的.
- 无论状态信号是什么, 都是通过一个函数来决定action的.
- 本书会介绍一些不使用value function的方法, 详细在第13章.  这种方法通过调整参数来使得提高policy 的性能. 对性能估计的过程中, agent是在和环境进行交互的.



### 1.5 An extended Example

